{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains some examples for how to use the finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this example, we load and print the Hugging Face URL for the best-model in the B7_MULTI_CHOICE competition.\"\"\"\n",
    "\n",
    "import finetune as ft\n",
    "from competitions.data import CompetitionId\n",
    "\n",
    "# Each model competes in a single competition. Find the best top performing miner UID for the \n",
    "# competition we care about (B7_MULTI_CHOICE, in this example).\n",
    "top_model_uid = ft.graph.best_uid(competition_id=CompetitionId.B7_MULTI_CHOICE)\n",
    "\n",
    "# Get the HuggingFace URL for this model.\n",
    "repo_url = await ft.mining.get_repo(top_model_uid)\n",
    "print(f\"The best-performing model for B7_MULTI_CHOICE competition is {repo_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this example, we load and print the actual metadata on the chain for the current best-model in the B7_MULTI_CHOICE competition.\"\"\"\n",
    "\n",
    "# NOTE: Run the first cell to get the top model uid first.\n",
    "import bittensor as bt\n",
    "\n",
    "import constants\n",
    "from taoverse.model.storage.chain.chain_model_metadata_store import (\n",
    "    ChainModelMetadataStore,\n",
    ")\n",
    "\n",
    "# Create a subtensor to communicate to the main chain.\n",
    "subtensor = bt.subtensor(network=\"finney\")\n",
    "\n",
    "# Find the hotkey of the current best uid from the metagraph.\n",
    "metagraph = subtensor.metagraph(constants.SUBNET_UID, lite=True)\n",
    "hotkey = metagraph.hotkeys[top_model_uid]\n",
    "\n",
    "# Instantiate the store that handles reading/writing metadata to the chain.\n",
    "metadata_store = ChainModelMetadataStore(subtensor, constants.SUBNET_UID)\n",
    "\n",
    "# Fetch the metadata, parsing from the chain payload into the ModelMetadata class.\n",
    "# NOTE: This may need to be retried due to transient chain failures.\n",
    "metadata = await metadata_store.retrieve_model_metadata(hotkey)\n",
    "\n",
    "# Breaking that down we have the following components.\n",
    "print(f\"HuggingFace namespace and repo name:  {metadata.id.namespace} and {metadata.id.name}\")\n",
    "print(f\"Exact commit for that HuggingFace repo: {metadata.id.commit}\")\n",
    "print(f\"ID of the competition this model is competing in: {metadata.id.competition_id}\")\n",
    "print(f\"Secure Hash of the model directory and the hotkey of the miner: {metadata.id.secure_hash}\")\n",
    "print(f\"block number that the metadata was committed to the chain: {metadata.block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this example, we load the top model for the B7_MULTI_CHOICE competition and converse with it.\"\"\"\n",
    "\n",
    "import bittensor as bt\n",
    "import torch\n",
    "from taoverse.model.competition import utils as competition_utils\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "import constants\n",
    "import finetune as ft\n",
    "from competitions.data import CompetitionId\n",
    "\n",
    "# The device to run the model on.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Download the top model to the specified directory.\n",
    "download_dir = \"./finetune-example\"\n",
    "model = await ft.mining.load_best_model(\n",
    "    download_dir=download_dir, competition_id=CompetitionId.B7_MULTI_CHOICE\n",
    ")\n",
    "\n",
    "# Load the competition so we can load the right tokenizer.\n",
    "metagraph = bt.metagraph(constants.SUBNET_UID)\n",
    "competition = competition_utils.get_competition_for_block(\n",
    "    CompetitionId.B7_MULTI_CHOICE,\n",
    "    metagraph.block,\n",
    "    constants.COMPETITION_SCHEDULE_BY_BLOCK,\n",
    ")\n",
    "tokenizer = ft.model.load_tokenizer(competition.constraints)\n",
    "\n",
    "# Decide on a prompt. (The answer to this prompt is B).\n",
    "prompt = \"\"\"According to Eratosthenes, who were the two most important philosophers of his age?\n",
    "\n",
    "A. Arcesilaus and Chrysippus\n",
    "B. Aristo and Arcesilaus\n",
    "C. Aristo and Zeno\n",
    "D. Zeno and Chrysippus\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Tokenize it.\n",
    "conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=competition.constraints.sequence_length,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Generate the output.\n",
    "# You may wish to customize the generation config.\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=20,\n",
    "    max_length=competition.constraints.sequence_length,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "response = ft.eval.method.generate_output(\n",
    "    model=model,\n",
    "    input_ids=input_ids,\n",
    "    generation_config=generation_config,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
