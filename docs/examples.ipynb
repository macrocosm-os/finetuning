{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains some examples for how to use the finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this example, we load and print the Hugging Face URL for the best-model in the SN9_MODEL competition.\"\"\"\n",
    "\n",
    "import finetune as ft\n",
    "from competitions.data import CompetitionId\n",
    "\n",
    "# Each model competes in a single competition. Find the best top performing miner UID for the \n",
    "# competition we care about (SN9_MODEL, in this example).\n",
    "top_model_uid = ft.graph.best_uid(competition_id=CompetitionId.SN9_MODEL)\n",
    "\n",
    "# Get the HuggingFace URL for this model.\n",
    "repo_url = await ft.mining.get_repo(top_model_uid)\n",
    "print(f\"The best-performing model for SN9_MODEL competition is {repo_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this example, we load and print the actual metadata on the chain for the current best-model in the SN9_MODEL competition.\"\"\"\n",
    "\n",
    "# NOTE: Run the first cell to get the top model uid first.\n",
    "import bittensor as bt\n",
    "\n",
    "import constants\n",
    "from model.storage.chain.chain_model_metadata_store import \\\n",
    "    ChainModelMetadataStore\n",
    "\n",
    "# Create a subtensor to communicate to the main chain.\n",
    "subtensor = bt.subtensor(network=\"finney\")\n",
    "\n",
    "# Find the hotkey of the current best uid from the metagraph.\n",
    "metagraph = subtensor.metagraph(constants.SUBNET_UID, lite=True)\n",
    "hotkey = metagraph.hotkeys[top_model_uid]\n",
    "\n",
    "# Instantiate the store that handles reading/writing metadata to the chain.\n",
    "metadata_store = ChainModelMetadataStore(subtensor)\n",
    "\n",
    "# Fetch the metadata, parsing from the chain payload into the ModelMetadata class.\n",
    "# NOTE: This may need to be retried due to transient chain failures.\n",
    "metadata = await metadata_store.retrieve_model_metadata(hotkey)\n",
    "\n",
    "# Breaking that down we have the following components.\n",
    "print(f\"HuggingFace namespace and repo name:  {metadata.id.namespace} and {metadata.id.name}\")\n",
    "print(f\"Exact commit for that HuggingFace repo: {metadata.id.commit}\")\n",
    "print(f\"ID of the competition this model is competing in: {metadata.id.competition_id.name}\")\n",
    "print(f\"Secure Hash of the model directory and the hotkey of the miner: {metadata.id.secure_hash}\")\n",
    "print(f\"block number that the metadata was committed to the chain: {metadata.block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this example, we load the top model for the SN9_MODEL competition and converse with it.\"\"\"\n",
    "\n",
    "import bittensor as bt\n",
    "import torch\n",
    "from taoverse.model.competition import utils as competition_utils\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "import constants\n",
    "import finetune as ft\n",
    "from competitions import utils as competition_utils\n",
    "from competitions.data import CompetitionId\n",
    "\n",
    "# The device to run the model on.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Download the top model to the specified directory.\n",
    "download_dir = \"./finetune-example\"\n",
    "model = await ft.mining.load_best_model(\n",
    "        download_dir=download_dir, competition_id=CompetitionId.SN9_MODEL\n",
    "    )\n",
    "\n",
    "# Load the competition so we can load the right tokenizer.\n",
    "metagraph = bt.metagraph(constants.SUBNET_UID)\n",
    "competition = competition_utils.get_competition_for_block(CompetitionId.SN9_MODEL, metagraph.block)\n",
    "tokenizer = ft.model.load_tokenizer(competition.constraints)\n",
    "\n",
    "# Decide on a prompt.\n",
    "prompt = \"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "\n",
    "# Tokenize it.\n",
    "conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=competition.constraints.sequence_length,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Generate the output.\n",
    "# You may wish to customize the generation config.\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=20,\n",
    "    max_length=competition.constraints.sequence_length,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "with torch.inference_mode():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids, generation_config=generation_config\n",
    "    )\n",
    "    response = tokenizer.decode(\n",
    "        output[0][len(input_ids[0]) :], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
